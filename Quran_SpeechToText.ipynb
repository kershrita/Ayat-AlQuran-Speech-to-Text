{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ckcz-jDV7BFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd8d98e-9b20-4533-bce2-7ff1c0662bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mltu librosa soundfile onnx tf2onnx ffmpeg\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y libsndfile1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDFu7GBeWZSE",
        "outputId": "d8719c44-3362-4a42-d23a-b288483f3296"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mltu\n",
            "  Downloading mltu-1.2.5-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.11/dist-packages (from mltu) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mltu) (4.67.1)\n",
            "Collecting qqdm==0.0.7 (from mltu)\n",
            "  Downloading qqdm-0.0.7.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mltu) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mltu) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from mltu) (4.11.0.86)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from mltu) (11.2.1)\n",
            "Collecting onnxruntime>=1.15.0 (from mltu)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mltu) (3.10.0)\n",
            "Collecting addict (from qqdm==0.0.7->mltu)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting jupyter (from qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (25.2.10)\n",
            "INFO: pip is looking at multiple versions of tf2onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading tf2onnx-1.15.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading tf2onnx-1.15.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting flatbuffers<3.0,>=1.12 (from tf2onnx)\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.14.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.15.0->mltu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.15.0->mltu) (1.13.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mltu) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mltu) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mltu) (2025.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.15.0->mltu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->qqdm==0.0.7->mltu) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->qqdm==0.0.7->mltu) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter->qqdm==0.0.7->mltu) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.15.0->mltu) (1.3.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->qqdm==0.0.7->mltu) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->qqdm==0.0.7->mltu) (3.0.15)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter->qqdm==0.0.7->mltu) (2.19.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->qqdm==0.0.7->mltu) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->qqdm==0.0.7->mltu) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->qqdm==0.0.7->mltu) (5.8.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->qqdm==0.0.7->mltu) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->qqdm==0.0.7->mltu) (75.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->qqdm==0.0.7->mltu) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->qqdm==0.0.7->mltu) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->qqdm==0.0.7->mltu) (1.3.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->qqdm==0.0.7->mltu) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->qqdm==0.0.7->mltu) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->qqdm==0.0.7->mltu) (4.9.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->qqdm==0.0.7->mltu) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (4.24.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->qqdm==0.0.7->mltu) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter->qqdm==0.0.7->mltu) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->qqdm==0.0.7->mltu) (2.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->qqdm==0.0.7->mltu) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->qqdm==0.0.7->mltu) (0.25.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->qqdm==0.0.7->mltu)\n",
            "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading mltu-1.2.5-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf2onnx-1.14.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: qqdm, ffmpeg\n",
            "  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qqdm: filename=qqdm-0.0.7-py3-none-any.whl size=6467 sha256=25ad96b8fa2c0d17001b8c9e7e891fb959f178263d56a64c04d4fe3c55c9ef17\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/3a/d0/e5a595714d658a67e14849528c6decc2bdc0274cf8542cb9ba\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=d97caa1c988bafe0e362166a29b690a3263811e9e224cbed48dffc8810987193\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
            "Successfully built qqdm ffmpeg\n",
            "Installing collected packages: flatbuffers, ffmpeg, addict, uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, onnx, json5, jedi, humanfriendly, fqdn, async-lru, tf2onnx, jupyter-server-terminals, jupyter-client, coloredlogs, arrow, onnxruntime, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, qqdm, mltu\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.2.10\n",
            "    Uninstalling flatbuffers-25.2.10:\n",
            "      Successfully uninstalled flatbuffers-25.2.10\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires flatbuffers>=24.3.25, but you have flatbuffers 2.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed addict-2.4.0 arrow-1.3.0 async-lru-2.0.5 coloredlogs-15.0.1 ffmpeg-1.4 flatbuffers-2.0.7 fqdn-1.5.1 humanfriendly-10.0 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-server-2.27.3 mltu-1.2.5 onnx-1.18.0 onnxruntime-1.22.0 overrides-7.7.0 python-json-logger-3.3.0 qqdm-0.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 tf2onnx-1.14.0 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,021 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,246 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,742 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,984 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,295 kB]\n",
            "Fetched 21.2 MB in 2s (8,895 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from mltu.dataProvider import DataProvider\n",
        "from mltu.transformers import SpectrogramPadding, LabelIndexer, LabelPadding\n",
        "from mltu.tensorflow.model_utils import residual_block, activation_layer\n",
        "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
        "from mltu.tensorflow.losses import CTCloss\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "from timeit import default_timer as timer\n",
        "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.data_adapters.py_dataset_adapter\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torchaudio\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.layers.activations.leaky_relu\")\n",
        "\n",
        "# Configuration class\n",
        "class Configs:\n",
        "    batch_size = 4\n",
        "    train_epochs = 15\n",
        "    frame_length = 512\n",
        "    frame_step = 256\n",
        "    fft_length = 512\n",
        "    target_sr = 44100\n",
        "    n_mels = 2\n",
        "    max_spectrogram_length = 1000  # Will be updated dynamically\n",
        "    max_text_length = 757\n",
        "    vocab = list(\"ءأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيًٌٍَُِّْٰٕٖٜٓٔٗٞٱۜ۠ۡۢۤۥۦۭۧۨ۬ \")\n",
        "    learning_rate = 0.0003\n",
        "    model_path = \"/content/drive/MyDrive/task1_ test mariam thesis model/model\"\n",
        "    input_shape = [None, 2]  # For n_mels=2\n",
        "    data_path = \"/content/drive/MyDrive/task1_ test mariam thesis model/data/csv/\"\n",
        "    spectrogram_path = \"/content/drive/MyDrive/spectrograms/\"\n",
        "\n",
        "configs = Configs()\n",
        "\n",
        "# Custom SpectrogramPadding\n",
        "class TruncatedSpectrogramPadding(SpectrogramPadding):\n",
        "    def __call__(self, spectrogram, label):\n",
        "        spectrogram = spectrogram.T  # From (n_mels, time_steps) to (time_steps, n_mels)\n",
        "        if spectrogram.shape[0] > self.max_spectrogram_length:\n",
        "            spectrogram = spectrogram[:self.max_spectrogram_length, :]\n",
        "        elif spectrogram.shape[0] < self.max_spectrogram_length:\n",
        "            spectrogram = np.pad(spectrogram,\n",
        "                                 ((0, self.max_spectrogram_length - spectrogram.shape[0]), (0, 0)),\n",
        "                                 mode=\"constant\",\n",
        "                                 constant_values=self.padding_value)\n",
        "        return spectrogram, label\n",
        "\n",
        "# Precompute spectrograms\n",
        "def precompute_spectrograms(dataset, output_dir, frame_length, frame_step, fft_length, target_sr, n_mels):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    spectrogram_paths = []\n",
        "    max_length = 0\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=target_sr,\n",
        "        n_fft=fft_length,\n",
        "        hop_length=frame_step,\n",
        "        win_length=frame_length,\n",
        "        n_mels=n_mels\n",
        "    ).to(\"cpu\")\n",
        "    db_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "    for idx, (wav_path, txt) in enumerate(dataset):\n",
        "        try:\n",
        "            wav_path = os.path.join(\"/content/drive/MyDrive/task1_ test mariam thesis model/data/audios\", os.path.basename(wav_path))\n",
        "            if not os.path.exists(wav_path):\n",
        "                print(f\"Audio file not found: {wav_path}\")\n",
        "                continue\n",
        "            audio, sr = torchaudio.load(wav_path)\n",
        "            if audio.dim() > 1 and audio.shape[0] > 1:\n",
        "                audio = torch.mean(audio, dim=0, keepdim=True)  # Shape: (1, samples)\n",
        "            audio = audio.squeeze(0)  # Shape: (samples,)\n",
        "            if sr != target_sr:\n",
        "                resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
        "                audio = resampler(audio)\n",
        "            spectrogram = mel_transform(audio)\n",
        "            spectrogram = db_transform(spectrogram).numpy()  # Shape: (n_mels, time_steps)\n",
        "            if spectrogram.shape[0] != n_mels:\n",
        "                raise ValueError(f\"Spectrogram {wav_path} has n_mels={spectrogram.shape[0]}, expected {n_mels}\")\n",
        "            max_length = max(max_length, spectrogram.shape[1])\n",
        "            spec_path = os.path.join(output_dir, f\"spectrogram_{idx}.npy\")\n",
        "            np.save(spec_path, spectrogram)\n",
        "            if not os.path.exists(spec_path):\n",
        "                raise IOError(f\"Failed to save spectrogram: {spec_path}\")\n",
        "            spectrogram_paths.append((spec_path, txt))\n",
        "            print(f\"Processed {wav_path}: shape {spectrogram.shape}, saved to {spec_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {wav_path}: {e}\")\n",
        "    return spectrogram_paths, max_length\n",
        "\n",
        "# Load precomputed spectrograms\n",
        "def load_precomputed_spectrogram(spec_path, txt):\n",
        "    if not os.path.exists(spec_path):\n",
        "        raise FileNotFoundError(f\"Spectrogram file not found: {spec_path}\")\n",
        "    spectrogram = np.load(spec_path)\n",
        "    if spectrogram.shape[0] != configs.n_mels:\n",
        "        raise ValueError(f\"Spectrogram {spec_path} has shape {spectrogram.shape}, expected n_mels={configs.n_mels}\")\n",
        "    return spectrogram, txt\n",
        "\n",
        "# DataProvider wrapper\n",
        "def data_provider_generator(data_provider):\n",
        "    for batch in data_provider:\n",
        "        inputs, targets = batch\n",
        "        yield np.array(inputs, dtype=np.float32), np.array(targets, dtype=np.int32)\n",
        "\n",
        "# DataProvider\n",
        "def convert_todata_provider(dataset, steps_per_epoch=None):\n",
        "    # Validate paths before creating DataProvider\n",
        "    valid_dataset = [(spec_path, txt) for spec_path, txt in dataset if os.path.exists(spec_path)]\n",
        "    if not valid_dataset:\n",
        "        raise FileNotFoundError(\"No valid spectrogram files found in dataset.\")\n",
        "    for spec_path, _ in valid_dataset[:5]:\n",
        "        print(f\"Validated spectrogram path: {spec_path}\")\n",
        "    data_provider = DataProvider(\n",
        "        dataset=valid_dataset,\n",
        "        skip_validation=False,\n",
        "        batch_size=configs.batch_size,\n",
        "        data_preprocessors=[load_precomputed_spectrogram],\n",
        "        transformers=[\n",
        "            TruncatedSpectrogramPadding(max_spectrogram_length=configs.max_spectrogram_length, padding_value=0),\n",
        "            LabelIndexer(configs.vocab),\n",
        "            LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
        "        ],\n",
        "    )\n",
        "    return data_provider_generator(data_provider), steps_per_epoch\n",
        "\n",
        "# Model architecture\n",
        "def train_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
        "    inputs = tf.keras.layers.Input(shape=input_dim, name=\"input\", dtype=tf.float32)\n",
        "    input = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), output_shape=lambda s: (s[0], s[1], s[2], 1))(inputs)\n",
        "\n",
        "    # Convolution layer 1\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\", use_bias=False)(input)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(negative_slope=0.1)(x)\n",
        "\n",
        "    # Convolution layer 2\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\", use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(negative_slope=0.1)(x)\n",
        "\n",
        "    # Reshape for RNN\n",
        "    x = tf.keras.layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "\n",
        "    # RNN layers\n",
        "    for _ in range(5):\n",
        "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "        x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Dense layer\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(negative_slope=0.1)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Classification layer\n",
        "    output = tf.keras.layers.Dense(output_dim + 1, activation=\"softmax\", dtype=tf.float32)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Load dataset\n",
        "csv_verses = \"someverse.csv\"  # Replace with your CSV file name\n",
        "df = pd.read_csv(os.path.join(configs.data_path, csv_verses))\n",
        "df['audio'] = df['audio'].str.replace('EqraTechCompany/tasks/', '', regex=False)\n",
        "\n",
        "# Split dataset\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Validation size: {len(val_df)}\")\n",
        "\n",
        "# Prepare metadata\n",
        "metadata_train_df = train_df.rename(columns={'audio': 'file_name', 'text': 'normalized_transcription'})[[\"file_name\", \"normalized_transcription\"]]\n",
        "metadata_val_df = val_df.rename(columns={'audio': 'file_name', 'text': 'normalized_transcription'})[[\"file_name\", \"normalized_transcription\"]]\n",
        "\n",
        "# Create dataset\n",
        "dataset_train = [[f\"{file}\", label.lower()] for file, label in metadata_train_df.values.tolist()]\n",
        "dataset_val = [[f\"{file}\", label.lower()] for file, label in metadata_val_df.values.tolist()]\n",
        "\n",
        "# Clear old spectrograms\n",
        "output_dir = configs.spectrogram_path\n",
        "if os.path.exists(output_dir):\n",
        "    for f in os.listdir(output_dir):\n",
        "        os.remove(os.path.join(output_dir, f))\n",
        "else:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Precompute spectrograms\n",
        "precomputed_train, train_max_length = precompute_spectrograms(\n",
        "    dataset_train, output_dir, configs.frame_length, configs.frame_step,\n",
        "    configs.fft_length, configs.target_sr, configs.n_mels\n",
        ")\n",
        "precomputed_val, val_max_length = precompute_spectrograms(\n",
        "    dataset_val, output_dir, configs.frame_length, configs.frame_step,\n",
        "    configs.fft_length, configs.target_sr, configs.n_mels\n",
        ")\n",
        "\n",
        "# Update max_spectrogram_length (limit to reduce memory usage)\n",
        "configs.max_spectrogram_length = min(max(train_max_length, val_max_length), 10000)  # Cap at 10000\n",
        "print(f\"Updated max_spectrogram_length: {configs.max_spectrogram_length}\")\n",
        "\n",
        "# Debug spectrogram shapes\n",
        "for spec_path, _ in precomputed_train[:5]:\n",
        "    spec = np.load(spec_path)\n",
        "    print(f\"Spectrogram {spec_path}: shape {spec.shape}\")\n",
        "\n",
        "# Create data providers\n",
        "train_data_generator, train_steps = convert_todata_provider(precomputed_train, steps_per_epoch=len(precomputed_train)//configs.batch_size)\n",
        "val_data_generator, val_steps = convert_todata_provider(precomputed_val, steps_per_epoch=len(precomputed_val)//configs.batch_size)\n",
        "\n",
        "# Debug data provider output\n",
        "def debug_data_provider(generator):\n",
        "    try:\n",
        "        for inputs, targets in generator:\n",
        "            print(f\"Input shape: {inputs.shape}, Target shape: {targets.shape}\")\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data provider: {e}\")\n",
        "debug_data_provider(train_data_generator)\n",
        "\n",
        "# Initialize and compile model\n",
        "model = train_model(\n",
        "    input_dim=configs.input_shape,\n",
        "    output_dim=len(configs.vocab),\n",
        "    dropout=0.5\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
        "    loss=CTCloss(),\n",
        "    metrics=[\n",
        "        CERMetric(vocabulary=configs.vocab),\n",
        "        WERMetric(vocabulary=configs.vocab)\n",
        "    ],\n",
        "    run_eagerly=False\n",
        ")\n",
        "model.summary(line_length=110)\n",
        "\n",
        "# Define callbacks\n",
        "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=10, verbose=1, mode=\"min\")\n",
        "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
        "trainLogger = TrainLogger(configs.model_path)\n",
        "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.8, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n",
        "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\")\n",
        "\n",
        "# Train the model\n",
        "start = timer()\n",
        "model.fit(\n",
        "    train_data_generator,\n",
        "    validation_data=val_data_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=val_steps,\n",
        "    epochs=configs.train_epochs,\n",
        "    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx]\n",
        ")\n",
        "elapsed_time_hours = (timer() - start) / 3600\n",
        "print(f\"Total time consumed: {elapsed_time_hours:.2f} hours.\")\n",
        "print(f\"Total time for {configs.train_epochs} epochs\")"
      ],
      "metadata": {
        "id": "zTf62rcbY-Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(f\"{configs.model_path}/final_model.h5\")"
      ],
      "metadata": {
        "id": "h0I9XeYzklWU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import tensorflow as tf\n",
        "from mltu.transformers import SpectrogramPadding\n",
        "from mltu.tensorflow.losses import CTCloss\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torchaudio\")\n",
        "\n",
        "# Configuration class (matched to training)\n",
        "class Configs:\n",
        "    frame_length = 512\n",
        "    frame_step = 256\n",
        "    fft_length = 512\n",
        "    target_sr = 44100\n",
        "    n_mels = 2\n",
        "    max_spectrogram_length = 10000  # Same as training cap\n",
        "    vocab = list(\"ءأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيًٌٍَُِّْٰٕٖٜٓٔٗٞٱۜ۠ۡۢۤۥۦۭۧۨ۬ \")\n",
        "    model_path = \"/content/drive/MyDrive/task1_ test mariam thesis model/model/model.h5\"  # Or use final_model.h5\n",
        "    blank_index = len(vocab)  # CTC blank token index\n",
        "\n",
        "configs = Configs()\n",
        "\n",
        "# Custom SpectrogramPadding (matched to training)\n",
        "class TruncatedSpectrogramPadding(SpectrogramPadding):\n",
        "    def __call__(self, spectrogram):\n",
        "        spectrogram = spectrogram.T  # From (n_mels, time_steps) to (time_steps, n_mels)\n",
        "        if spectrogram.shape[0] > self.max_spectrogram_length:\n",
        "            spectrogram = spectrogram[:self.max_spectrogram_length, :]\n",
        "        elif spectrogram.shape[0] < self.max_spectrogram_length:\n",
        "            spectrogram = np.pad(spectrogram,\n",
        "                                 ((0, self.max_spectrogram_length - spectrogram.shape[0]), (0, 0)),\n",
        "                                 mode=\"constant\",\n",
        "                                 constant_values=self.padding_value)\n",
        "        return spectrogram\n",
        "\n",
        "# Custom CTC Greedy Decoder\n",
        "def ctc_greedy_decoder(logits, vocab, blank_index):\n",
        "    \"\"\"\n",
        "    Decode CTC logits using greedy decoding.\n",
        "    Args:\n",
        "        logits: np.array of shape (batch, time_steps, vocab_size + 1)\n",
        "        vocab: List of characters\n",
        "        blank_index: Index of the blank token\n",
        "    Returns:\n",
        "        List of decoded strings\n",
        "    \"\"\"\n",
        "    # Get the argmax at each time step\n",
        "    predicted_ids = np.argmax(logits, axis=-1)  # Shape: (batch, time_steps)\n",
        "\n",
        "    decoded_texts = []\n",
        "    for batch_idx in range(predicted_ids.shape[0]):\n",
        "        sequence = predicted_ids[batch_idx]\n",
        "        # Collapse repeats and remove blanks\n",
        "        prev_id = None\n",
        "        decoded = []\n",
        "        for id_ in sequence:\n",
        "            if id_ != prev_id and id_ != blank_index:\n",
        "                decoded.append(vocab[id_])\n",
        "            prev_id = id_\n",
        "        decoded_texts.append(''.join(decoded))\n",
        "\n",
        "    return decoded_texts\n",
        "\n",
        "# Process WAV file to spectrogram\n",
        "def process_wav_to_spectrogram(wav_path):\n",
        "    try:\n",
        "        if not os.path.exists(wav_path):\n",
        "            raise FileNotFoundError(f\"WAV file not found: {wav_path}\")\n",
        "\n",
        "        audio, sr = torchaudio.load(wav_path)\n",
        "        # Convert to mono\n",
        "        if audio.dim() > 1 and audio.shape[0] > 1:\n",
        "            audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "        audio = audio.squeeze(0)  # Shape: (samples,)\n",
        "\n",
        "        if sr != configs.target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(sr, configs.target_sr)\n",
        "            audio = resampler(audio)\n",
        "\n",
        "        mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=configs.target_sr,\n",
        "            n_fft=configs.fft_length,\n",
        "            hop_length=configs.frame_step,\n",
        "            win_length=configs.frame_length,\n",
        "            n_mels=configs.n_mels\n",
        "        ).to(\"cpu\")\n",
        "        db_transform = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "        spectrogram = mel_transform(audio)\n",
        "        spectrogram = db_transform(spectrogram).numpy()  # Shape: (n_mels, time_steps)\n",
        "\n",
        "        if spectrogram.shape[0] != configs.n_mels:\n",
        "            raise ValueError(f\"Spectrogram has n_mels={spectrogram.shape[0]}, expected {configs.n_mels}\")\n",
        "\n",
        "        print(f\"Processed {wav_path}: spectrogram shape {spectrogram.shape}\")\n",
        "\n",
        "        # Apply padding\n",
        "        padder = TruncatedSpectrogramPadding(max_spectrogram_length=configs.max_spectrogram_length, padding_value=0)\n",
        "        spectrogram = padder(spectrogram)\n",
        "\n",
        "        # Add batch dimension\n",
        "        spectrogram = np.expand_dims(spectrogram, axis=0)  # Shape: (1, max_spectrogram_length, n_mels)\n",
        "\n",
        "        return spectrogram\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {wav_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load model and predict\n",
        "def predict_transcription(model, spectrogram):\n",
        "    try:\n",
        "        # Predict logits\n",
        "        logits = model.predict(spectrogram, verbose=0)  # Shape: (1, time_steps, vocab_size + 1)\n",
        "\n",
        "        # CTC greedy decoding\n",
        "        decoded = ctc_greedy_decoder(logits, configs.vocab, configs.blank_index)\n",
        "\n",
        "        return decoded[0]  # Return first (and only) transcription\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main test function\n",
        "def test_model(wav_path):\n",
        "    # Load model\n",
        "    if not os.path.exists(configs.model_path):\n",
        "        print(f\"Model file not found: {configs.model_path}\")\n",
        "        print(\"Trying final_model.h5...\")\n",
        "        configs.model_path = configs.model_path.replace(\"model.h5\", \"final_model.h5\")\n",
        "        if not os.path.exists(configs.model_path):\n",
        "            raise FileNotFoundError(f\"Final model file not found: {configs.model_path}\")\n",
        "\n",
        "    model = tf.keras.models.load_model(configs.model_path, custom_objects={\"CTCloss\": CTCloss})\n",
        "    print(f\"Loaded model from {configs.model_path}\")\n",
        "\n",
        "    # Process WAV file\n",
        "    spectrogram = process_wav_to_spectrogram(wav_path)\n",
        "    if spectrogram is None:\n",
        "        return\n",
        "\n",
        "    # Predict transcription\n",
        "    transcription = predict_transcription(model, spectrogram)\n",
        "    if transcription is None:\n",
        "        return\n",
        "\n",
        "    print(f\"\\nPredicted Transcription: {transcription}\")\n",
        "\n",
        "# Example usage\n",
        "wav_file = \"/content/drive/MyDrive/task1_ test mariam thesis model/data/audios/Mosab_Mohammad_067024.wav\"  # Replace with your WAV file path\n",
        "test_model(wav_file)"
      ],
      "metadata": {
        "id": "PQuKTdHUY-MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kceo9BYjkzey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}